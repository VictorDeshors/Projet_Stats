{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiexA9tKkVWf",
        "outputId": "5135a015-a1b0-42ad-d3dc-9993b8d6c188"
      },
      "outputs": [],
      "source": [
        "#!pip install hawkesbook\n",
        "#!pip install newsapi-python\n",
        "#!pip install pytrends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YqpUO_SrySy"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import requests # Utilisé pour les requêtes manuelles à Polygon\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import time\n",
        "from pytrends.request import TrendReq\n",
        "import pandas_datareader.data as web\n",
        "# NLTK n'est plus nécessaire car nous n'utilisons plus NewsAPI pour le sentiment\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import statsmodels.api as sm\n",
        "import itertools\n",
        "import os\n",
        "import warnings\n",
        "import hawkesbook as hl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjGpwuXBr6S5"
      },
      "outputs": [],
      "source": [
        "# --- Configuration ---\n",
        "# ATTENTION: Remplacez par votre vraie clé API !\n",
        "POLYGON_API_KEY = \"YOUR API KEY\" # Mettez votre clé ici\n",
        "# NEWSAPI_KEY n'est plus nécessaire\n",
        "\n",
        "# Actions NASDAQ (Exemple)\n",
        "TICKERS = ['AAPL', 'MSFT', 'AMZN', 'NVDA', 'TSLA', 'MRNA', 'SBUX', 'CMCSA']\n",
        "N_STOCKS = len(TICKERS)\n",
        "\n",
        "# Période d'analyse\n",
        "END_DATE = dt.date.today()\n",
        "START_DATE = END_DATE - dt.timedelta(days=365*2) # 1 an\n",
        "\n",
        "# Paramètres temporels\n",
        "TIME_STEP = 5 # minutes\n",
        "ROLLING_VOL_WINDOW_DAYS = 7\n",
        "EPISODE_DURATION_WEEKS = 1\n",
        "PAGINATION_RANGE_DAYS = 30 # Récupérer les données Polygon par tranches de 30 jours pour éviter les limites\n",
        "\n",
        "# Paramètres de détection d'événements\n",
        "VOLATILITY_THRESHOLD_QUANTILE = 0.92\n",
        "\n",
        "# Paramètres Hawkes (pour l'estimation MLE)\n",
        "INITIAL_LAMBDA = np.ones(N_STOCKS) * 0.1\n",
        "INITIAL_ALPHA = np.ones((N_STOCKS, N_STOCKS)) * 0.05\n",
        "np.fill_diagonal(INITIAL_ALPHA, 0.1)\n",
        "INITIAL_BETA = np.ones(N_STOCKS) * 1.0\n",
        "INITIAL_THETA = (INITIAL_LAMBDA, INITIAL_ALPHA, INITIAL_BETA)\n",
        "\n",
        "# Indicateurs Macroéconomiques (FRED) - Ajout de VXN\n",
        "FRED_INDICATORS = {\n",
        "    'VIX': 'VIXCLS',       # Indice de Volatilité CBOE\n",
        "    'VXN': 'VXNCLS',       # Indice de Volatilité NASDAQ-100\n",
        "    'UNRATE': 'UNRATE',    # Taux de chômage US\n",
        "    'CPI': 'CPIAUCSL',     # Indice des prix à la consommation US\n",
        "    'FEDFUNDS': 'FEDFUNDS' # Taux des fonds fédéraux\n",
        "}\n",
        "\n",
        "# Keywords pour Google Trends - Ajout de comparaisons par paire\n",
        "TRENDS_KEYWORDS_PER_STOCK = {ticker: [ticker] for ticker in TICKERS}\n",
        "TRENDS_KEYWORDS_THEMES = [\n",
        "    'semiconductor shortage', 'AI regulation', 'cloud computing competition', 'supply chain disruption',\n",
        "    'Apple vs Microsoft', 'Nvidia vs AMD', 'Tesla vs Ford' # Exemples de paires\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g1uLdK9sD7c"
      },
      "outputs": [],
      "source": [
        "# --- Fonctions ---\n",
        "\n",
        "def fetch_polygon_data_manual(tickers, start_date, end_date, api_key, timespan='minute', multiplier=5, pagination_days=30):\n",
        "    \"\"\"Récupère les données de prix historiques de Polygon.io manuellement avec requests, gère la pagination et les limites de taux.\"\"\"\n",
        "    print(f\"Récupération manuelle des données Polygon pour {len(tickers)} tickers...\")\n",
        "    base_url = \"https://api.polygon.io\"\n",
        "    all_ticker_data = {ticker: [] for ticker in tickers}\n",
        "    requests_count = 0\n",
        "    minute_start_time = time.time()\n",
        "\n",
        "    # Headers pour l'authentification\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    # Boucle sur les tickers\n",
        "    for ticker in tqdm(tickers, desc=\"Tickers\"):\n",
        "        current_start_date = start_date\n",
        "        # Boucle de pagination par date\n",
        "        while current_start_date <= end_date:\n",
        "            # Définir la fin de la période de pagination\n",
        "            current_end_date = min(current_start_date + dt.timedelta(days=pagination_days - 1), end_date)\n",
        "            start_str = current_start_date.strftime(\"%Y-%m-%d\")\n",
        "            end_str = current_end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "            # Construire l'URL de l'endpoint\n",
        "            endpoint = f\"/v2/aggs/ticker/{ticker}/range/{multiplier}/{timespan}/{start_str}/{end_str}\"\n",
        "            url = base_url + endpoint\n",
        "            params = {\"adjusted\": \"true\", \"sort\": \"asc\", \"limit\": 50000} # Max limit\n",
        "\n",
        "            # --- Gestion de la limite de taux (5 requêtes/minute pour le plan gratuit) ---\n",
        "            current_time = time.time()\n",
        "            if requests_count >= 4 and (current_time - minute_start_time) < 60:\n",
        "                sleep_time = 60 - (current_time - minute_start_time) + 1 # +1 pour la marge\n",
        "                print(f\"Limite de taux atteinte, attente de {sleep_time:.1f} secondes...\")\n",
        "                time.sleep(sleep_time)\n",
        "                requests_count = 0\n",
        "                minute_start_time = time.time()\n",
        "            elif (current_time - minute_start_time) >= 60:\n",
        "                 requests_count = 0\n",
        "                 minute_start_time = time.time()\n",
        "            # --------------------------------------------------------------------------\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, params=params)\n",
        "                requests_count += 1\n",
        "                response.raise_for_status()\n",
        "                data = response.json()\n",
        "\n",
        "                if data.get(\"resultsCount\", 0) > 0 and 'results' in data:\n",
        "                    df = pd.DataFrame(data['results'])\n",
        "                    df = df.rename(columns={'t': 'timestamp', 'c': ticker})\n",
        "                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)\n",
        "                    df = df.set_index('timestamp')\n",
        "                    all_ticker_data[ticker].append(df[[ticker]])\n",
        "                # Même si les résultats sont vides, on avance la date\n",
        "                current_start_date = current_end_date + dt.timedelta(days=1)\n",
        "                # Petite pause entre chaque requête individuelle pour être sûr\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Erreur lors de la requête pour {ticker} ({start_str} à {end_str}): {e}\")\n",
        "                if response is not None:\n",
        "                     print(f\"Status Code: {response.status_code}, Response: {response.text[:200]}\") # Affiche début de la réponse si erreur\n",
        "                     if response.status_code == 429: # Rate limit explicite\n",
        "                         print(\"Rate limit (429) détecté, attente de 65 secondes...\")\n",
        "                         time.sleep(65)\n",
        "                         requests_count = 0 # Réinitialiser le compteur après attente\n",
        "                         minute_start_time = time.time()\n",
        "                         continue # Retenter la même requête\n",
        "                # Si autre erreur, on saute cette période pour ce ticker mais on continue\n",
        "                current_start_date = current_end_date + dt.timedelta(days=1)\n",
        "                time.sleep(2) # Pause après une erreur\n",
        "\n",
        "            except Exception as e_json:\n",
        "                 print(f\"Erreur de décodage JSON ou autre pour {ticker} ({start_str} à {end_str}): {e_json}\")\n",
        "                 # Si erreur, on saute cette période pour ce ticker mais on continue\n",
        "                 current_start_date = current_end_date + dt.timedelta(days=1)\n",
        "                 time.sleep(2)\n",
        "\n",
        "\n",
        "    # Combiner les données pour chaque ticker puis tous les tickers\n",
        "    final_dfs = []\n",
        "    for ticker in tickers:\n",
        "        if all_ticker_data[ticker]:\n",
        "            ticker_df = pd.concat(all_ticker_data[ticker]).sort_index()\n",
        "            # Supprimer les duplicatas d'index si la pagination a des chevauchements minimes\n",
        "            ticker_df = ticker_df[~ticker_df.index.duplicated(keep='first')]\n",
        "            final_dfs.append(ticker_df)\n",
        "        else:\n",
        "            print(f\"Aucune donnée valide récupérée pour {ticker}\")\n",
        "\n",
        "    if not final_dfs:\n",
        "        print(\"Aucune donnée n'a pu être récupérée pour aucun ticker.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Combiner tous les tickers\n",
        "    full_df = pd.concat(final_dfs, axis=1)\n",
        "    # Remplir les NaNs (peut arriver si les tickers n'ont pas exactement les mêmes timestamps)\n",
        "    full_df = full_df.ffill().bfill()\n",
        "    print(\"Données Polygon récupérées manuellement.\")\n",
        "    return full_df\n",
        "\n",
        "\n",
        "def calculate_volatility_and_events(price_df, window_days, timestep_min, quantile_threshold):\n",
        "    \"\"\"Calcule la volatilité et détecte les événements de forte volatilité.\"\"\"\n",
        "    print(\"Calcul de la volatilité et détection des événements...\")\n",
        "    log_returns = np.log(price_df / price_df.shift(1)).dropna()\n",
        "\n",
        "    trading_periods_per_day = (6.5 * 60) / timestep_min # Approximation heures de trading (ex: 9:30-16:00 ET)\n",
        "    window_size = int(window_days * trading_periods_per_day)\n",
        "\n",
        "    if window_size <= 1:\n",
        "        raise ValueError(\"Fenêtre de volatilité trop petite.\")\n",
        "\n",
        "    rolling_vol = log_returns.rolling(window=window_size, min_periods=int(window_size*0.8)).std().dropna() # min_periods ajouté\n",
        "\n",
        "    events = []\n",
        "    thresholds = {}\n",
        "    for ticker in price_df.columns:\n",
        "        if ticker not in rolling_vol.columns or rolling_vol[ticker].isnull().all():\n",
        "            print(f\"Pas de données de volatilité pour {ticker}, il est ignoré.\")\n",
        "            continue\n",
        "        vol = rolling_vol[ticker]\n",
        "        try:\n",
        "            threshold = vol.quantile(quantile_threshold)\n",
        "            thresholds[ticker] = threshold\n",
        "        except Exception as e:\n",
        "             print(f\"Impossible de calculer le quantile pour {ticker}, seuil mis à NaN: {e}\")\n",
        "             thresholds[ticker] = np.nan\n",
        "             continue # Ne pas chercher d'événements si seuil non calculable\n",
        "\n",
        "        if pd.isna(threshold): continue\n",
        "\n",
        "        high_vol_times = vol[vol > threshold].index\n",
        "        ticker_id = list(price_df.columns).index(ticker)\n",
        "        for t in high_vol_times:\n",
        "            time_in_days = (t - price_df.index[0]).total_seconds() / (24 * 60 * 60)\n",
        "            events.append({'timestamp': t, 'time_days': time_in_days, 'ticker': ticker, 'ticker_id': ticker_id})\n",
        "\n",
        "    print(f\"Détection de {len(events)} événements de forte volatilité.\")\n",
        "    if not events:\n",
        "        print(\"Aucun événement détecté.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), {}\n",
        "\n",
        "    events_df = pd.DataFrame(events).sort_values('timestamp').reset_index(drop=True)\n",
        "    return rolling_vol, events_df, thresholds\n",
        "\n",
        "# Fonction pour récupérer les tendances Google - inchangée mais utilisera les keywords étendus\n",
        "def fetch_trends_data(keywords_dict, themes, start_date, end_date, episode_starts):\n",
        "    \"\"\"Récupère les données Google Trends agrégées par épisode.\"\"\"\n",
        "    print(\"Récupération des données Google Trends...\")\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    trends_data = []\n",
        "\n",
        "    all_keywords = list(itertools.chain(*keywords_dict.values())) + themes\n",
        "    # Attention : certains keywords peuvent ne retourner aucune donnée.\n",
        "    # Google Trends limite à 5 keywords par requête\n",
        "    kw_groups = [all_keywords[i:i + 5] for i in range(0, len(all_keywords), 5)]\n",
        "\n",
        "    trends_df = pd.DataFrame()\n",
        "    timeframe = f\"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}\"\n",
        "\n",
        "    for kw_group in tqdm(kw_groups, desc=\"Fetching Trends\"):\n",
        "        # Filtrer les éventuels keywords vides ou None\n",
        "        kw_group = [kw for kw in kw_group if kw]\n",
        "        if not kw_group: continue\n",
        "        try:\n",
        "            pytrends.build_payload(kw_group, cat=0, timeframe=timeframe, geo='', gprop='')\n",
        "            interest_over_time_df = pytrends.interest_over_time()\n",
        "            if 'isPartial' in interest_over_time_df.columns:\n",
        "                 interest_over_time_df = interest_over_time_df.drop(columns=['isPartial'])\n",
        "\n",
        "            if trends_df.empty:\n",
        "                trends_df = interest_over_time_df\n",
        "            else:\n",
        "                # Join en gérant les colonnes manquantes dans l'un ou l'autre\n",
        "                trends_df = trends_df.join(interest_over_time_df, how='outer')\n",
        "            time.sleep(2)\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur Pytrends pour {kw_group}: {e}.\")\n",
        "            # Ajouter des colonnes vides si nécessaire pour la cohérence\n",
        "            for kw in kw_group:\n",
        "                if kw not in trends_df.columns: trends_df[kw] = np.nan\n",
        "\n",
        "    if trends_df.empty:\n",
        "        print(\"Aucune donnée Google Trends trouvée.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Agréger par épisode (moyenne)\n",
        "    # Note : resample fonctionne mieux avec un index DatetimeIndex\n",
        "    trends_df.index = pd.to_datetime(trends_df.index)\n",
        "    trends_agg = trends_df.resample(f'{EPISODE_DURATION_WEEKS*7}D').mean()\n",
        "\n",
        "    # Aligner sur les débuts d'épisodes et remplir\n",
        "    trends_agg = trends_agg.reindex(episode_starts, method='ffill').fillna(0)\n",
        "    trends_agg.columns = ['Trend_'+col.replace(' ', '_') for col in trends_agg.columns] # Remplacer espaces\n",
        "\n",
        "    print(\"Données Google Trends agrégées.\")\n",
        "    return trends_agg\n",
        "\n",
        "# Fonction pour récupérer les données macro FRED - inchangée mais utilisera les indicateurs étendus\n",
        "def fetch_macro_data(fred_indicators, start_date, end_date, episode_starts):\n",
        "    \"\"\"Récupère les données macroéconomiques de FRED agrégées par épisode.\"\"\"\n",
        "    print(\"Récupération des données macroéconomiques FRED...\")\n",
        "    try:\n",
        "        # Essayer de récupérer les données\n",
        "        macro_df = web.DataReader(list(fred_indicators.values()), 'fred', start_date, end_date)\n",
        "        # Renommer les colonnes avec les clés fournies\n",
        "        name_map = {v: k for k, v in fred_indicators.items()}\n",
        "        macro_df = macro_df.rename(columns=name_map)\n",
        "\n",
        "        # Agréger par épisode (prendre la dernière valeur)\n",
        "        # Assurer que l'index est un DatetimeIndex\n",
        "        macro_df.index = pd.to_datetime(macro_df.index)\n",
        "        macro_agg = macro_df.resample(f'{EPISODE_DURATION_WEEKS*7}D').last()\n",
        "\n",
        "        # Aligner sur les débuts d'épisodes et remplir les NaNs\n",
        "        macro_agg = macro_agg.reindex(episode_starts, method='ffill')\n",
        "        macro_agg = macro_agg.fillna(method='ffill').fillna(method='bfill') # Fill forward puis backward\n",
        "\n",
        "        macro_agg.columns = ['Macro_'+col for col in macro_agg.columns]\n",
        "        print(\"Données macroéconomiques agrégées.\")\n",
        "        return macro_agg\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur lors de la récupération des données FRED: {e}\")\n",
        "        # Retourner un DataFrame vide avec les colonnes attendues si erreur\n",
        "        cols = ['Macro_'+k for k in fred_indicators.keys()]\n",
        "        return pd.DataFrame(columns=cols, index=episode_starts)\n",
        "\n",
        "# Nouvelle fonction pour calculer les corrélations glissantes par épisode\n",
        "def calculate_rolling_correlations(price_df, episode_starts):\n",
        "    \"\"\"Calcule la corrélation des log-rendements entre les paires d'actions pour chaque épisode.\"\"\"\n",
        "    print(\"Calcul des corrélations par épisode...\")\n",
        "    log_returns = np.log(price_df / price_df.shift(1)).dropna()\n",
        "    correlation_features = []\n",
        "\n",
        "    ticker_pairs = list(itertools.combinations(price_df.columns, 2))\n",
        "    column_names = [f\"Corr_{p[0]}_{p[1]}\" for p in ticker_pairs]\n",
        "\n",
        "    for i in range(len(episode_starts)):\n",
        "        ep_start = episode_starts[i]\n",
        "        ep_end = episode_starts[i+1] if i+1 < len(episode_starts) else log_returns.index.max() + dt.timedelta(microseconds=1)\n",
        "\n",
        "        episode_returns = log_returns[(log_returns.index >= ep_start) & (log_returns.index < ep_end)]\n",
        "\n",
        "        correlations = {}\n",
        "        if len(episode_returns) > 1: # Besoin d'au moins 2 points pour la corrélation\n",
        "            corr_matrix = episode_returns.corr()\n",
        "            for pair in ticker_pairs:\n",
        "                col_name = f\"Corr_{pair[0]}_{pair[1]}\"\n",
        "                # Vérifier si les tickers existent dans la matrice (peuvent être absents si pas de données)\n",
        "                if pair[0] in corr_matrix.columns and pair[1] in corr_matrix.columns:\n",
        "                     correlations[col_name] = corr_matrix.loc[pair[0], pair[1]]\n",
        "                else:\n",
        "                     correlations[col_name] = np.nan # Mettre NaN si un ticker manque\n",
        "        else:\n",
        "             for pair in ticker_pairs:\n",
        "                col_name = f\"Corr_{pair[0]}_{pair[1]}\"\n",
        "                correlations[col_name] = np.nan # Mettre NaN si pas assez de données\n",
        "\n",
        "        correlations['episode_start'] = ep_start\n",
        "        correlation_features.append(correlations)\n",
        "\n",
        "    if not correlation_features:\n",
        "        return pd.DataFrame(columns=['episode_start'] + column_names).set_index('episode_start')\n",
        "\n",
        "    corr_df = pd.DataFrame(correlation_features).set_index('episode_start')\n",
        "    # Remplir les NaNs (par ex., si épisode avec < 2 retours) avec 0 ou moyenne ? Prenons 0.\n",
        "    corr_df = corr_df.fillna(0)\n",
        "    print(\"Corrélations par épisode calculées.\")\n",
        "    return corr_df\n",
        "\n",
        "\n",
        "# Fonction d'estimation Hawkes - inchangée\n",
        "def estimate_hawkes_for_episode(events_in_episode, n_stocks, T_episode_days, initial_theta):\n",
        "    \"\"\"Estime les paramètres Hawkes pour un épisode donné.\"\"\"\n",
        "    if events_in_episode.empty or len(events_in_episode) < 2:\n",
        "        nan_params = np.full(n_stocks + n_stocks**2 + n_stocks, np.nan)\n",
        "        return nan_params\n",
        "\n",
        "    times = events_in_episode['time_days'].values\n",
        "    ids = events_in_episode['ticker_id'].values\n",
        "    times = times - times[0]\n",
        "    T = max(T_episode_days, times[-1] + 1e-6) # Assurer T >= max(time)\n",
        "\n",
        "    try:\n",
        "        # print(f\"Estimating MLE for episode (T={T:.2f} days, N={len(times)} events)...\")\n",
        "        estimated_theta, log_likelihood = hl.mutual_exp_mle(times, ids, T, initial_theta)\n",
        "        # print(f\"Log-Likelihood: {log_likelihood:.2f}\")\n",
        "        flat_theta = hl.flatten_theta(estimated_theta)\n",
        "        return flat_theta\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur durant l'estimation MLE: {e}\")\n",
        "        nan_params = np.full(n_stocks + n_stocks**2 + n_stocks, np.nan)\n",
        "        return nan_params\n",
        "\n",
        "# Fonction de plot - inchangée\n",
        "def plot_point_process(events_df, tickers):\n",
        "    \"\"\"Affiche le point process multivarié.\"\"\"\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    # Utiliser les timestamps directement pour l'axe des x\n",
        "    plt.scatter(events_df['timestamp'], events_df['ticker_id'], alpha=0.6, s=10)\n",
        "    plt.yticks(range(len(tickers)), tickers)\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Action\")\n",
        "    plt.title(\"Processus Ponctuel des Événements de Forte Volatilité (7 jours)\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AkOa8KIsN4m",
        "outputId": "92efc511-244f-4232-de33-64bbca726947"
      },
      "outputs": [],
      "source": [
        "# 1. Récupérer les données de prix (version manuelle)\n",
        "price_data = fetch_polygon_data_manual(TICKERS, START_DATE, END_DATE, POLYGON_API_KEY,\n",
        "                                     multiplier=TIME_STEP, pagination_days=PAGINATION_RANGE_DAYS)\n",
        "\n",
        "if price_data.empty or price_data.shape[1] < N_STOCKS:\n",
        "    print(f\"Données de prix incomplètes ou manquantes (obtenues: {price_data.shape[1]}/{N_STOCKS}). Arrêt.\")\n",
        "    exit()\n",
        "\n",
        "# S'assurer que l'index est trié\n",
        "price_data = price_data.sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rrQATRXyMpX",
        "outputId": "48d401d5-94a3-4d6c-916c-c6514927d2e0"
      },
      "outputs": [],
      "source": [
        "print(type(price_data))\n",
        "print(price_data.shape)\n",
        "print(365 * 10 * 12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "PMScwOYYy9wV",
        "outputId": "d6e4acfe-b3f9-49e1-97a7-c98f9d917b4e"
      },
      "outputs": [],
      "source": [
        "# 2. Calculer la volatilité et détecter les événements\n",
        "rolling_vol, events_df, vol_thresholds = calculate_volatility_and_events(\n",
        "    price_data,\n",
        "    ROLLING_VOL_WINDOW_DAYS,\n",
        "    TIME_STEP,\n",
        "    VOLATILITY_THRESHOLD_QUANTILE\n",
        ")\n",
        "\n",
        "if events_df.empty:\n",
        "    print(\"Impossible de continuer sans événements de volatilité.\")\n",
        "    exit()\n",
        "\n",
        "# Afficher le processus ponctuel\n",
        "plot_point_process(events_df, list(price_data.columns)) # Utiliser les colonnes réelles de price_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Bf-S3Y4zFlH",
        "outputId": "25cc24a6-1a3a-4d71-8814-e5fb3acecaf5"
      },
      "outputs": [],
      "source": [
        "# 3. Définir les épisodes de 2 semaines\n",
        "episode_duration_td = dt.timedelta(weeks=EPISODE_DURATION_WEEKS)\n",
        "# S'assurer que les épisodes commencent bien après le début des données pour la vol glissante\n",
        "start_offset = dt.timedelta(days=ROLLING_VOL_WINDOW_DAYS + 1) # Ajouter une marge\n",
        "analysis_start_date = price_data.index.min() + start_offset\n",
        "episode_starts = pd.date_range(start=analysis_start_date.normalize(),\n",
        "                               end=price_data.index.max().normalize(),\n",
        "                               freq=episode_duration_td)\n",
        "\n",
        "if episode_starts.empty:\n",
        "     print(\"Aucun épisode complet trouvé après la période initiale de calcul de volatilité. Vérifiez les dates.\")\n",
        "     exit()\n",
        "\n",
        "print(f\"Nombre d'épisodes de {EPISODE_DURATION_WEEKS} semaines: {len(episode_starts)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqYwj1A2zIpH",
        "outputId": "1032802c-0148-4698-a9c4-69a0012a6ce4"
      },
      "outputs": [],
      "source": [
        "# 4. Récupérer et agréger les indicateurs alternatifs\n",
        "trends_agg = fetch_trends_data(TRENDS_KEYWORDS_PER_STOCK, TRENDS_KEYWORDS_THEMES, START_DATE, END_DATE, episode_starts)\n",
        "macro_agg = fetch_macro_data(FRED_INDICATORS, START_DATE, END_DATE, episode_starts)\n",
        "corr_agg = calculate_rolling_correlations(price_data, episode_starts)\n",
        "\n",
        "# Fusionner tous les indicateurs agrégés\n",
        "# news_agg est retiré\n",
        "all_indicators = pd.concat([trends_agg, macro_agg, corr_agg], axis=1)\n",
        "\n",
        "# Remplir les NaNs restants (peut arriver si une API a eu des erreurs)\n",
        "all_indicators = all_indicators.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
        "\n",
        "print(\"Indicateurs agrégés:\")\n",
        "#print(all_indicators.head())\n",
        "print(all_indicators.shape)\n",
        "print(\"Colonnes d'indicateurs:\", all_indicators.columns)\n",
        "\n",
        "\n",
        "# --- Note sur GDELT ---\n",
        "#print(\"\\nNote: Pour une analyse d'actualités open-source plus poussée (co-mentions, sentiment),\")\n",
        "#print(\"considérez GDELT Project. Son intégration demande un travail de traitement de données conséquent.\")\n",
        "# ---------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfiFmy9j2ugg",
        "outputId": "75a53279-8fc5-4537-a303-c4aabfdb14d7"
      },
      "outputs": [],
      "source": [
        "len(episode_starts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpm2QR23zaWL",
        "outputId": "26ebd081-b37c-456c-9e95-83a4673e4607"
      },
      "outputs": [],
      "source": [
        "# 5. Estimer les paramètres Hawkes pour chaque épisode\n",
        "estimated_params_list = []\n",
        "episode_data_for_regression = []\n",
        "\n",
        "# Utiliser une copie des colonnes réelles pour l'ID ticker\n",
        "actual_tickers = list(price_data.columns)\n",
        "ticker_to_id = {ticker: i for i, ticker in enumerate(actual_tickers)}\n",
        "events_df['ticker_id'] = events_df['ticker'].map(ticker_to_id) # Recalculer ID basé sur les tickers présents\n",
        "\n",
        "# Ajuster N_STOCKS si certains tickers manquaient\n",
        "N_STOCKS_ACTUAL = len(actual_tickers)\n",
        "if N_STOCKS_ACTUAL != N_STOCKS:\n",
        "    print(f\"Ajustement du nombre d'actions à {N_STOCKS_ACTUAL} en raison de données manquantes.\")\n",
        "    # Recréer les paramètres initiaux avec la bonne dimension si nécessaire\n",
        "    INITIAL_LAMBDA = np.ones(N_STOCKS_ACTUAL) * 0.1\n",
        "    INITIAL_ALPHA = np.ones((N_STOCKS_ACTUAL, N_STOCKS_ACTUAL)) * 0.05\n",
        "    np.fill_diagonal(INITIAL_ALPHA, 0.1)\n",
        "    INITIAL_BETA = np.ones(N_STOCKS_ACTUAL) * 1.0\n",
        "    INITIAL_THETA = (INITIAL_LAMBDA, INITIAL_ALPHA, INITIAL_BETA)\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(episode_starts)), desc=\"Estimating Hawkes per Episode\"):\n",
        "    ep_start_ts = episode_starts[i]\n",
        "    ep_end_ts = episode_starts[i+1] if i+1 < len(episode_starts) else price_data.index.max() + dt.timedelta(microseconds=1)\n",
        "\n",
        "    mask = (events_df['timestamp'] >= ep_start_ts) & (events_df['timestamp'] < ep_end_ts)\n",
        "    events_in_episode = events_df[mask].copy() # Utiliser une copie pour éviter SettingWithCopyWarning\n",
        "\n",
        "    if events_in_episode.empty:\n",
        "        # print(f\"Pas d'événements pour l'épisode commençant le {ep_start_ts}, ignoré.\")\n",
        "        continue\n",
        "\n",
        "    T_episode = (ep_end_ts - ep_start_ts).total_seconds() / (24 * 60 * 60)\n",
        "\n",
        "    flat_params = estimate_hawkes_for_episode(events_in_episode, N_STOCKS_ACTUAL, T_episode, INITIAL_THETA)\n",
        "\n",
        "    if not np.isnan(flat_params).any() and ep_start_ts in all_indicators.index:\n",
        "         episode_data_for_regression.append({\n",
        "             'episode_start': ep_start_ts,\n",
        "             'hawkes_params': flat_params,\n",
        "             'indicators': all_indicators.loc[ep_start_ts].values\n",
        "         })\n",
        "    # else:\n",
        "        # Gérer les cas d'échec ou d'indicateurs manquants si nécessaire (déjà loggé dans estimate_hawkes...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4BmxUuB2C1Y",
        "outputId": "24566371-6f72-4cdc-9c4a-b6ed132ca733"
      },
      "outputs": [],
      "source": [
        "print(type(episode_data_for_regression))\n",
        "print(len(episode_data_for_regression)) # 17 -> 17 épisodes de 2 semaines durant lesquels le processus contient des evenements\n",
        "print(type(episode_data_for_regression[0]))\n",
        "print(episode_data_for_regression[0].keys())\n",
        "print(type(episode_data_for_regression[0]['episode_start']))\n",
        "print(type(episode_data_for_regression[0]['hawkes_params']))\n",
        "print(type(episode_data_for_regression[0]['indicators']))\n",
        "\n",
        "H1 = episode_data_for_regression[0]\n",
        "print(len(H1))\n",
        "\n",
        "print(H1['indicators'].shape)\n",
        "print(H1['hawkes_params'].shape)\n",
        "np.set_printoptions(precision=5, suppress=True)\n",
        "pd.set_option('display.float_format', '{:.5f}'.format)\n",
        "\n",
        "print(H1['hawkes_params'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XepGE_HMAZzJ",
        "outputId": "4febdadd-2bc6-4c67-983f-2bad23d43d84"
      },
      "outputs": [],
      "source": [
        "# 6. Préparer les données pour la régression\n",
        "if not episode_data_for_regression:\n",
        "    print(\"Aucune donnée valide pour entraîner le modèle de régression.\")\n",
        "    exit()\n",
        "\n",
        "X_list = [item['indicators'] for item in episode_data_for_regression]\n",
        "Y_list = [item['hawkes_params'] for item in episode_data_for_regression]\n",
        "\n",
        "# S'assurer que toutes les lignes d'indicateurs ont la même forme\n",
        "expected_len = len(all_indicators.columns)\n",
        "X_list_filtered = [x for x in X_list if len(x) == expected_len]\n",
        "Y_list_filtered = [y for x, y in zip(X_list, Y_list) if len(x) == expected_len]\n",
        "\n",
        "if len(X_list_filtered) != len(X_list):\n",
        "     print(f\"Avertissement: {len(X_list) - len(X_list_filtered)} épisodes exclus en raison d'un nombre incohérent d'indicateurs.\")\n",
        "\n",
        "if not X_list_filtered:\n",
        "     print(\"Aucun indicateur cohérent trouvé pour la régression.\")\n",
        "     exit()\n",
        "\n",
        "X = pd.DataFrame(X_list_filtered, columns=all_indicators.columns)\n",
        "# Créer des noms pour les paramètres Hawkes (potentiellement ajustés par N_STOCKS_ACTUAL)\n",
        "param_names = [f'lambda_{i}' for i in range(N_STOCKS_ACTUAL)] + \\\n",
        "              [f'alpha_{i}_{j}' for i in range(N_STOCKS_ACTUAL) for j in range(N_STOCKS_ACTUAL)] + \\\n",
        "              [f'beta_{i}' for i in range(N_STOCKS_ACTUAL)]\n",
        "Y = pd.DataFrame(Y_list_filtered, columns=param_names)\n",
        "\n",
        "print(\"\\nPréparation des données pour la régression:\")\n",
        "print(\"X (indicateurs) shape:\", X.shape)\n",
        "print(\"Y (paramètres Hawkes) shape:\", Y.shape)\n",
        "# print(\"X head:\\n\", X.head())\n",
        "# print(\"Y head:\\n\", Y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C2-3rAurkTEb",
        "outputId": "8c3b342e-baca-42f2-f70a-6de9541f4646"
      },
      "outputs": [],
      "source": [
        "# --- Exécution Principale ---\n",
        "\n",
        "\n",
        "# 7. Entraîner le modèle de régression et valider\n",
        "\n",
        "# Standardiser les features X\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "X_scaled_const = sm.add_constant(X_scaled, has_constant='add') # Ajoute une colonne de 1s\n",
        "\n",
        "regression_results = {}\n",
        "\n",
        "# Exemple : Prédire le premier terme d'interaction alpha_0_1\n",
        "target_param = f'alpha_{0}_{1}' if N_STOCKS_ACTUAL > 1 else f'alpha_{0}_{0}' # Assurer que l'index existe\n",
        "\n",
        "if target_param not in Y.columns:\n",
        "    print(f\"Erreur: Paramètre cible '{target_param}' introuvable dans Y.\")\n",
        "    print(\"Paramètres disponibles:\", list(Y.columns))\n",
        "    exit()\n",
        "\n",
        "# Vérifier qu'il y a plus d'observations que de prédicteurs\n",
        "if X_scaled_const.shape[0] <= X_scaled_const.shape[1]:\n",
        "    print(f\"Pas assez d'observations ({X_scaled_const.shape[0]}) pour le nombre de prédicteurs ({X_scaled_const.shape[1]}). Régression impossible.\")\n",
        "    exit()\n",
        "\n",
        "y_target = Y[target_param]\n",
        "\n",
        "# Séparer train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled_const, y_target, test_size=0.2, random_state=42, shuffle=False) # Ne pas mélanger pour les séries temporelles\n",
        "\n",
        "print(f\"\\nEntraînement du modèle de régression pour {target_param}...\")\n",
        "\n",
        "# Utilisation de statsmodels\n",
        "try:\n",
        "    model = sm.OLS(y_train, X_train)\n",
        "    results = model.fit()\n",
        "    print(f\"\\n--- Résultats de la Régression OLS pour {target_param} ---\")\n",
        "    print(results.summary())\n",
        "    y_train_pred = results.predict(X_train)\n",
        "    train_rmse = np.sqrt(np.mean((y_train - y_train_pred)**2))\n",
        "\n",
        "    # Évaluation sur le set de test\n",
        "    # Assurer que X_test a les mêmes colonnes que X_train\n",
        "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
        "    y_pred = results.predict(X_test)\n",
        "    test_rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
        "    print(f\"\\nRMSE sur l'ensemble d'entraînement pour {target_param}: {train_rmse:.4f}\")\n",
        "    print(f\"\\nRMSE sur l'ensemble de test pour {target_param}: {test_rmse:.4f}\")\n",
        "    print(f\"Moyenne de {target_param} (test): {y_test.mean():.4f}\")\n",
        "    print(f\"Écart-type de {target_param} (test): {y_test.std():.4f}\")\n",
        "\n",
        "    # Plot des résidus vs prédits (simple vérification)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(results.predict(X_train), results.resid)\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel(\"Valeurs Prédites (Train)\")\n",
        "    plt.ylabel(\"Résidus (Train)\")\n",
        "    plt.title(f\"Résidus vs Prédits pour {target_param} (Train set)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print((y_test- y_pred)**2)\n",
        "    print((y_train_pred - y_train)**2)\n",
        "\n",
        "except Exception as e_reg:\n",
        "     print(f\"Erreur lors de l'entraînement/évaluation de la régression: {e_reg}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Fin du script ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmXpkCrXArTo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
